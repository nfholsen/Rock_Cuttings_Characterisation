{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create resized dataset notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description : \n",
    "\n",
    "Table of contents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuttings import *\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_resized(test_name,\n",
    "                    label,\n",
    "                     list_images,\n",
    "                     date,\n",
    "                     size=128,\n",
    "                     train=True,\n",
    "                     test=False,\n",
    "                     folder_data=\"Cuttings_data/\"):\n",
    "    \n",
    "    test_path = test_name+\"/\"\n",
    "    \n",
    "    # List all the images in the folder\n",
    "    train_tiffs = os.listdir(folder_data+test_path)\n",
    "    \n",
    "    if train:\n",
    "        name_csv = 'train_'+date+'.csv'\n",
    "        path_cut = './train/cuttings_'+date+'/'\n",
    "        path_csv = './train/csv/'\n",
    "        \n",
    "    if test:\n",
    "        name_csv = 'test_'+date+'.csv'\n",
    "        path_cut = \"./test/cuttings_\"+date+\"/\"\n",
    "        path_csv = \"./test/csv/\"\n",
    "    \n",
    "    # create folders if not already existing\n",
    "    try :\n",
    "        os.listdir(path_cut)\n",
    "    except FileNotFoundError : \n",
    "        os.mkdir(path_cut) \n",
    "        print(\"Directory '%s' created\" %path_cut) \n",
    "    try :\n",
    "        os.listdir(path_csv)\n",
    "    except FileNotFoundError : \n",
    "        os.mkdir(path_csv) \n",
    "        print(\"Directory '%s' created\" %path_csv) \n",
    "        # Need to create empty csv file with all the features\n",
    "        df = pd.DataFrame(columns=['path',\n",
    "                                   'image_name',\n",
    "                                   'rock_type',\n",
    "                                   'scan_name',\n",
    "                                   'label_assigned',\n",
    "                                   'area',\n",
    "                                   'centroid',\n",
    "                                   'bbox',\n",
    "                                   'bbox_rect'])\n",
    "        df.to_csv(path_csv+name_csv)\n",
    "        \n",
    "    # Import CSV\n",
    "    df_metadata = pd.read_csv(path_csv+name_csv,index_col=0)\n",
    "    \n",
    "    WIDTH = size\n",
    "    HEIGHT = size\n",
    "    \n",
    "    # Iterate over all the images \n",
    "    for image_i in list_images :\n",
    "        \n",
    "        # Extract the image number\n",
    "        tiff = train_tiffs[image_i]\n",
    "        \n",
    "        cut = Cuttings(folder_data+test_path,tiff)\n",
    "        \n",
    "        image = cut.load_picture()\n",
    "        \n",
    "        mask = cut.assign_mask(image)\n",
    "        \n",
    "        dilated = cut.assign_label(mask)\n",
    "        \n",
    "        big_samples = cut.big_cuttings(dilated)\n",
    "        \n",
    "        label = label\n",
    "        \n",
    "        # Create dataframe to store the metadata for 1 scan\n",
    "        df_scan = pd.DataFrame()\n",
    "        \n",
    "        # Create lists to store the metadata for 1 scan\n",
    "        list_path = []\n",
    "        list_image_name = []\n",
    "        list_rock_type = []\n",
    "        list_scan_name = []\n",
    "        list_label_assigned = []\n",
    "        list_area = []\n",
    "        list_local_centroid = []\n",
    "        list_bbox = []\n",
    "        list_bbox_rect = []\n",
    "        \n",
    "        # Iterate on all the cuttings found on the pictures\n",
    "        for sample_i in big_samples:\n",
    "            \n",
    "            # Extract the cuttings\n",
    "            im_rect = img_as_ubyte(dilated == regionprops(dilated)[sample_i].label)\n",
    "            contours,_ = cv2.findContours(im_rect, 1, 2)\n",
    "            rect = cv2.minAreaRect(contours[0])\n",
    "            if len(contours) >= 2:\n",
    "                cmax = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "                rect = cv2.minAreaRect(cmax)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            # Rotated rectangle bounding box coordinates\n",
    "            box = np.int0(box)\n",
    "            \n",
    "            # Extract ractangular padded cutting\n",
    "            cutting_to_save = cv2.resize(crop_rectangle(image*im_rect,box,rect),\n",
    "                   (WIDTH,HEIGHT), \n",
    "                   interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            # Save cutting (original + mask)\n",
    "            # Cutting\n",
    "            cv2.imwrite(path_cut+test_name+'_'+train_tiffs[image_i][:-4]+'_'+str(sample_i)+'.png', cutting_to_save)\n",
    "\n",
    "            # Save metadata : \n",
    "            # Path\n",
    "            list_path.append(path_cut+test_name+'_'+train_tiffs[image_i][:-4]+'_'+str(sample_i)+'.png')\n",
    "            # Image name \n",
    "            list_image_name.append(test_name+'_'+train_tiffs[image_i][:-4]+'_'+str(sample_i))\n",
    "            # Rock type \n",
    "            list_rock_type.append(label)\n",
    "            # Scan name \n",
    "            list_scan_name.append(train_tiffs[image_i][:-4])\n",
    "            # Label_assigned\n",
    "            list_label_assigned.append(sample_i)\n",
    "            # Area\n",
    "            list_area.append(regionprops(dilated)[sample_i].area)\n",
    "            # Local Centroid\n",
    "            list_local_centroid.append(regionprops(dilated)[sample_i].local_centroid)\n",
    "            # Bbox\n",
    "            minr, minc, maxr, maxc = regionprops(dilated)[sample_i].bbox\n",
    "            list_bbox.append([minr, minc, maxr, maxc])\n",
    "            # Bbox_rect\n",
    "            minr_r, minc_r, maxr_r, maxc_r = list(box[0]),list(box[1]),list(box[2]),list(box[3])\n",
    "            list_bbox_rect.append([ minr_r, minc_r, maxr_r, maxc_r])\n",
    "            \n",
    "        df_scan['path'] = list_path\n",
    "        df_scan['image_name'] = list_image_name\n",
    "        df_scan['rock_type'] = list_rock_type\n",
    "        df_scan['scan_name'] = list_scan_name\n",
    "        df_scan['label_assigned'] = list_label_assigned\n",
    "        df_scan['area'] = list_area\n",
    "        df_scan['centroid'] = list_local_centroid\n",
    "        df_scan['bbox'] = list_bbox\n",
    "        df_scan['bbox_rect'] = list_bbox_rect\n",
    "        \n",
    "        df_metadata = df_metadata.append(df_scan,ignore_index=True)\n",
    "    df_metadata.to_csv(path_csv+name_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>label</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>list_start</th>\n",
       "      <th>list_end</th>\n",
       "      <th>list_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ML-DB-Geo1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>360</td>\n",
       "      <td>700</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML-DB-Geo2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML-DB-Geo3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>300</td>\n",
       "      <td>550</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS-DB-Geo01-1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>900</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MS-DB-Geo01-2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>160</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_name  label  train   test  list_start  list_end  list_step\n",
       "0     ML-DB-Geo1      0   True  False         360       700          4\n",
       "1     ML-DB-Geo2      0   True  False         120       600          4\n",
       "2     ML-DB-Geo3      0  False   True         300       550          4\n",
       "3  MS-DB-Geo01-1      1   True  False         200       900         10\n",
       "4  MS-DB-Geo01-2      1   True  False         160       800         10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_config = pd.read_excel('config_file.xlsx')\n",
    "df_config.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset - 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"resized_128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './train/cuttings_resized_128/' created\n",
      "Directory './train/csv_resized_128/' created\n",
      "Directory './test/cuttings_resized_128/' created\n",
      "Directory './test/csv_resized_128/' created\n"
     ]
    }
   ],
   "source": [
    "for i in range(df_config.shape[0]):\n",
    "    dataset_resized(df_config.loc[i,'test_name'],\n",
    "                df_config.loc[i,'label'],\n",
    "                np.arange(df_config.loc[i,'list_start'], df_config.loc[i,'list_end'], df_config.loc[i,'list_step']).tolist(),\n",
    "                date,\n",
    "                size=128,\n",
    "                train=df_config.loc[i,'train'],\n",
    "                test=df_config.loc[i,'test'],\n",
    "                folder_data='/Users/nilso/Documents/EPFL/MA4/PDS Turberg/Cuttings_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of rock per category :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Number of rock type 0 = 1846\n",
      "Number of rock type 1 = 1033\n",
      "Number of rock type 2 = 1660\n",
      "Number of rock type 3 = 1497\n",
      "Number of rock type 4 = 1374\n",
      "\n",
      "Test\n",
      "Number of rock type 0 = 709\n",
      "Number of rock type 1 = 517\n",
      "Number of rock type 2 = 559\n",
      "Number of rock type 3 = 656\n",
      "Number of rock type 4 = 398\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train/csv_'+date+'/train_'+date+'.csv',index_col=0)\n",
    "df_test = pd.read_csv('test/csv_'+date+'/test_'+date+'.csv',index_col=0)\n",
    "\n",
    "# Train\n",
    "print('Train')\n",
    "for i in range(5):\n",
    "    print('Number of rock type {} = {}'.format(i,df_train[df_train['rock_type'] == i].shape[0]))\n",
    "print()\n",
    "# Test\n",
    "print('Test')\n",
    "for i in range(5):\n",
    "    print('Number of rock type {} = {}'.format(i,df_test[df_test['rock_type'] == i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create uniform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 samples per rock type\n",
    "list_im_train = []\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    list_im_train+=random.sample(list(df_train[df_train['rock_type'] ==i].index.values),k = 1000)\n",
    "\n",
    "# Save training/validation dataset\n",
    "df_train.iloc[list_im_train].sort_index().reset_index(drop=True).to_csv('train/csv_'+date+'/train_'+date+'_final.csv')\n",
    "\n",
    "# Generate 200 samples per rock type\n",
    "list_im_test = []\n",
    "for i in range(5):\n",
    "    list_im_test+=random.sample(list(df_test[df_test['rock_type'] ==i].index.values),k = 200)\n",
    "\n",
    "# Save test dataset\n",
    "df_test.loc[list_im_test].sort_index().reset_index(drop=True).to_csv('test/csv_'+date+'/test_'+date+'_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Mean and Std for standarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxNormalization(image,vmin=0, vmax=1):\n",
    "        arr = np.array(image).astype('float32')\n",
    "        arr = (arr - arr.min()) / (arr.max() - arr.min())\n",
    "        arr = (vmax - vmin) * arr + vmin\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean :\n",
      "0.5157003\n",
      "\n",
      "Std :\n",
      "0.32948261\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train/csv_'+date+'/'+'train_'+date+'_final.csv',index_col=0)\n",
    "\n",
    "vec_im = []\n",
    "for im in df['path']:\n",
    "    cut = Cuttings(im[:24],im[24:])\n",
    "    image = cut.load_picture()\n",
    "    image = MinMaxNormalization(image,vmin=0, vmax=1)\n",
    "    vec_im += list(image.reshape(-1))\n",
    "    \n",
    "print('Mean :')\n",
    "print(np.mean(vec_im))\n",
    "print()\n",
    "print('Std :')\n",
    "print(np.std(vec_im))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

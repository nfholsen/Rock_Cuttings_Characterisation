{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuttings import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pickle \n",
    "import os.path\n",
    "import time\n",
    "import copy\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils import data \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tf\n",
    "from torchvision import models # torchvision for pre-trained models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalization(object):\n",
    "    \"\"\"\n",
    "    Normalized (Min-Max) the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, vmin=0, vmax=1):\n",
    "        \"\"\"\n",
    "        Constructor of the grayscale transform.\n",
    "        ----------\n",
    "        INPUT\n",
    "            |---- vmin (float / int) the desired minimum value.\n",
    "            |---- vmax (float / int) the desired maximum value.\n",
    "        OUTPUT\n",
    "            |---- None\n",
    "        \"\"\"\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "\n",
    "    def __call__(self, image, mask=None):\n",
    "        \"\"\"\n",
    "        Apply a Min-Max Normalization to the image.\n",
    "        ----------\n",
    "        INPUT\n",
    "            |---- image (PIL.Image) the image to normalize.\n",
    "        OUTPUT\n",
    "            |---- image (np.array) the normalized image.\n",
    "        \"\"\"\n",
    "        arr = np.array(image).astype('float32')\n",
    "        arr = (arr - arr.min()) / (arr.max() - arr.min())\n",
    "        arr = (self.vmax - self.vmin) * arr + self.vmin\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cuttings_Dataset(data.Dataset):\n",
    "    def __init__(self, sample_df, data_path, mean, std, resize,data_augmentation=True):\n",
    "        \"\"\"\n",
    "        Constructor of the dataset.\n",
    "        \"\"\"\n",
    "        data.Dataset.__init__(self)\n",
    "        self.sample_df = sample_df\n",
    "        self.data_path = data_path\n",
    "        self.data_augmentation = data_augmentation\n",
    "        \n",
    "        self.transform =  tf.Compose([tf.Grayscale(num_output_channels=3),\n",
    "                                        MinMaxNormalization(),\n",
    "                                        tf.ToTensor(),\n",
    "                                        tf.Normalize((mean,mean,mean),(std,std,std))])\n",
    "        \n",
    "        if data_augmentation:\n",
    "            self.transform = tf.Compose([tf.Grayscale(num_output_channels=3),\n",
    "                                            tf.RandomVerticalFlip(p=0.5),\n",
    "                                            tf.RandomHorizontalFlip(p=0.5),\n",
    "                                            tf.RandomRotation([-90,90],resample=False, expand=False, center=None, fill=None),\n",
    "                                            MinMaxNormalization(),\n",
    "                                            tf.ToTensor(),\n",
    "                                            tf.Normalize((mean,mean,mean),(std,std,std))])\n",
    "        \n",
    "        if resize:\n",
    "            self.transform = tf.Compose([tf.Grayscale(num_output_channels=3),\n",
    "                                            tf.Resize(224),\n",
    "                                            tf.RandomVerticalFlip(p=0.5),\n",
    "                                            tf.RandomHorizontalFlip(p=0.5),\n",
    "                                            tf.RandomRotation([-90,90],resample=False, expand=False, center=None, fill=None),\n",
    "                                            MinMaxNormalization(),\n",
    "                                            tf.ToTensor(),\n",
    "                                            tf.Normalize((mean,mean,mean),(std,std,std))])\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.sample_df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset.\n",
    "        \"\"\"\n",
    "        # load image\n",
    "        im = Image.open(self.data_path + self.sample_df.loc[idx,'path'])\n",
    "        # load label\n",
    "        label = torch.tensor(self.sample_df.loc[idx,'rock_type'])\n",
    "        \n",
    "        im = self.transform(im)\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['ML', 'MS','BL','GN','OL'])\n",
    "\n",
    "def DataLoader(root,train_path,test_path,batch_size,validation_size=0.2,**kwargs):\n",
    "    df = pd.read_csv(train_path,index_col=0)\n",
    "\n",
    "    train, val = train_test_split(df,test_size=validation_size,random_state=0,stratify=df['rock_type'])\n",
    "\n",
    "    cuttings_datasets = {}\n",
    "\n",
    "    cuttings_datasets['train'] = Cuttings_Dataset(train.reset_index(drop=True),root,\n",
    "                                                    mean=kwargs.get('mean'),\n",
    "                                                    std=kwargs.get('std'),\n",
    "                                                    resize=kwargs.get('resize'),\n",
    "                                                    data_augmentation=True)\n",
    "    \n",
    "    cuttings_datasets['val'] = Cuttings_Dataset(val.reset_index(drop=True),root,\n",
    "                                                    mean=kwargs.get('mean'),\n",
    "                                                    std=kwargs.get('std'),\n",
    "                                                    resize=kwargs.get('resize'),\n",
    "                                                    data_augmentation=False)\n",
    "    \n",
    "    cuttings_datasets['test'] = Cuttings_Dataset(pd.read_csv(test_path,index_col=0),root,\n",
    "                                                    mean=kwargs.get('mean'),\n",
    "                                                    std=kwargs.get('std'),\n",
    "                                                    resize=kwargs.get('resize'),\n",
    "                                                    data_augmentation=False)\n",
    "\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(cuttings_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "                    for x in ['train', 'val', 'test']}\n",
    "\n",
    "    dataset_sizes = {x: len(cuttings_datasets[x]) for x in ['train','val','test']}\n",
    "    return dataloaders, cuttings_datasets,dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cuttings_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-104f089faead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuttings_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cuttings_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(cuttings_datasets['train'].__getitem__(1)[0].view([3,128,128])[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuttings_datasets['train'].__getitem__(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model,num_classes, layers_to_unfreeze, use_pretrained=True):\n",
    "    \n",
    "    # By default, when we load a pretrained model all of the parameters have .requires_grad=True\n",
    "    if model =='resnet34':\n",
    "        model = models.resnet34(pretrained=use_pretrained)\n",
    "    if model =='resnet18':\n",
    "        model = models.resnet18(pretrained=use_pretrained)\n",
    "    \n",
    "    if len(layers_to_unfreeze)>0:\n",
    "        set_parameter_requires_grad(model, layers_to_unfreeze)\n",
    "        \n",
    "    num_ftrs = model.fc.in_features\n",
    "    \n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model,layers_to_unfreeze):\n",
    "    \n",
    "    for name, child in model.named_children():\n",
    "        if name in layers_to_unfreeze:\n",
    "            print(name + ' is unfrozen')\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            print(name + ' is frozen')\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, checkpoint, PATH,i, num_epochs=25):\n",
    "    \n",
    "    # First time training the model\n",
    "    if checkpoint == None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        last_epoch = 1\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        val_loss = []\n",
    "        val_accuracy = []\n",
    "        \n",
    "        best_acc = 0.0\n",
    "\n",
    "    # We are already training the model\n",
    "    else :\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler = checkpoint['scheduler']\n",
    "        best_model_wts = checkpoint['best_model_state_dict']\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        train_loss = checkpoint['loss']\n",
    "        train_accuracy = checkpoint['accuracy']\n",
    "        val_loss = checkpoint['val_loss']\n",
    "        val_accuracy = checkpoint['val_accuracy']\n",
    "        \n",
    "        best_acc = max(val_accuracy)\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    \n",
    "    for epoch in range(last_epoch,num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        print('LR:', scheduler.get_lr())\n",
    "\n",
    "        running_loss_train = 0.0\n",
    "        running_corrects_train = 0\n",
    "            \n",
    "        model.train()# Set model to training mode\n",
    "        \n",
    "        # Iterate over data Training\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            inputs.require_grad = True\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss_train += loss.item() * inputs.size(0)\n",
    "            running_corrects_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss_train = running_loss_train / dataset_sizes[\"train\"]\n",
    "        epoch_acc_train = running_corrects_train.double() / dataset_sizes[\"train\"]\n",
    "        \n",
    "        train_loss.append(epoch_loss_train)\n",
    "        train_accuracy.append(epoch_acc_train)\n",
    "        \n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                \"Train\", epoch_loss_train, epoch_acc_train))\n",
    "        \n",
    "        running_loss_val = 0.0\n",
    "        running_corrects_val = 0\n",
    "            \n",
    "        model.eval() # Set model to evaluate mode\n",
    "        \n",
    "        # Iterate over data Evaluation\n",
    "        for inputs, labels in dataloaders[\"val\"]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "                loss = criterion(outputs, labels)   \n",
    "\n",
    "            # statistics\n",
    "            running_loss_val += loss.item() * inputs.size(0)\n",
    "            running_corrects_val += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss_val = running_loss_val / dataset_sizes[\"val\"]\n",
    "        epoch_acc_val = running_corrects_val.double() / dataset_sizes[\"val\"]\n",
    "        \n",
    "        val_loss.append(epoch_loss_val)\n",
    "        val_accuracy.append(epoch_acc_val)\n",
    "        \n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                \"Val\", epoch_loss_val, epoch_acc_val))\n",
    "            \n",
    "        # deep copy the model\n",
    "        if epoch_acc_val > best_acc:\n",
    "            best_acc = epoch_acc_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        print()\n",
    "        torch.save({\n",
    "            'epoch': epoch+last_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_model_state_dict': best_model_wts,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "            'accuracy': train_accuracy,\n",
    "            'val_loss':val_loss,\n",
    "            'val_accuracy':val_accuracy,\n",
    "            'scheduler': scheduler,\n",
    "            'model_number':i,\n",
    "            }, PATH)\n",
    "        print('Model Saved')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_loss, train_accuracy, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_file(ini_file):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize the parser\n",
    "    parser = ConfigParser()\n",
    "    parser.read(os.path.abspath(ini_file))\n",
    "    \n",
    "    return dict(parser.items( \"inputs\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediciton\n",
    "def prediciton(model):\n",
    "    preds_vec = []\n",
    "    true_vec = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            preds_vec+=preds.tolist()\n",
    "            true_vec+=labels.tolist()\n",
    "    return preds_vec, true_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = 'inputs.ini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file exists\n",
      "Checkpoint file exists\n",
      "Restart at epoch : 5\n",
      "False\n",
      "Dataloader ok !\n",
      "\n",
      "conv1 is frozen\n",
      "bn1 is frozen\n",
      "relu is frozen\n",
      "maxpool is frozen\n",
      "layer1 is frozen\n",
      "layer2 is frozen\n",
      "layer3 is frozen\n",
      "layer4 is frozen\n",
      "avgpool is frozen\n",
      "fc is unfrozen\n",
      "\n",
      "Layers to be trained :\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\n",
      "\n",
      "Device : cuda:0\n",
      "--------------\n",
      "Start training\n",
      "--------------\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "LR: [1.0000000000000004e-08]\n",
      "Train Loss: 0.7867 Acc: 0.6997\n",
      "Val Loss: 0.8720 Acc: 0.6570\n",
      "\n",
      "Model Saved\n",
      "\n",
      "Training complete in 0m 24s\n",
      "Best val Acc: 0.670000\n"
     ]
    }
   ],
   "source": [
    "# # Check input file\n",
    "if os.path.isfile(INPUT_PATH):\n",
    "    print (\"Config file exists\")\n",
    "    inputs = read_config_file(INPUT_PATH)\n",
    "else:\n",
    "    print (\"Config file does not exist\")\n",
    "\n",
    "PATH_model_partial = inputs.get('model_root')+\\\n",
    "        inputs.get('model_name')+\\\n",
    "        inputs.get('path')\n",
    "\n",
    "if os.path.isfile(PATH_model_partial):\n",
    "    print (\"Checkpoint file exists\")\n",
    "    checkpoint = torch.load(PATH_model_partial)\n",
    "    print('Restart at epoch :',checkpoint.get('epoch'))\n",
    "    model_number = int(checkpoint.get('model_number'))\n",
    "else:\n",
    "    print (\"Checkpoint file does not exist\")\n",
    "    model_number = 0\n",
    "    checkpoint = None\n",
    "\n",
    "# # Assign inputs parameters \n",
    "root = inputs.get('root')\n",
    "train_path = root + inputs.get('train_path')\n",
    "test_path = root + inputs.get('test_path')\n",
    "\n",
    "batch_size = int(inputs.get('batch_size'))\n",
    "epoch_tot = int(inputs.get('epoch_tot'))\n",
    "\n",
    "\n",
    "if inputs.get('resize') == 'False':\n",
    "    resize = False\n",
    "    print(resize)\n",
    "elif inputs.get('resize') == 'True':\n",
    "    resize = True\n",
    "    print(resize)\n",
    "else :\n",
    "    print('Wrong bool format, resize set to False')\n",
    "    resize = False\n",
    "\n",
    "config ={\\\n",
    "        'mean':float(inputs.get('mean')),\n",
    "        'std':float(inputs.get('std')),\n",
    "        'resize':resize\\\n",
    "        }\n",
    "\n",
    "layers_to_unfreeze = list(inputs.get('layers_to_unfreeeze').split(','))\n",
    "\n",
    "# # Create dataloaders\n",
    "dataloaders, cuttings_datasets, dataset_sizes = DataLoader(root,\n",
    "                                                           train_path,\n",
    "                                                           test_path,\n",
    "                                                           batch_size,\n",
    "                                                           validation_size=0.2,\n",
    "                                                           **config)\n",
    "\n",
    "print('Dataloader ok !')\n",
    "print()\n",
    "\n",
    "model_type = inputs.get('model')\n",
    "\n",
    "# Loop start here\n",
    "for i in range(model_number,2) : \n",
    "    \n",
    "    # # Create model \n",
    "    model = initialize_model(model_type,\n",
    "                             len(classes),\n",
    "                             layers_to_unfreeze)\n",
    "    \n",
    "    print('\\nLayers to be trained :')\n",
    "    # Visualize layers to be trained\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    \n",
    "    print()\n",
    "    # Pass model to gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print('\\nDevice :', device)\n",
    "    \n",
    "    print('--------------')\n",
    "    print('Start training')\n",
    "    print('--------------')\n",
    "    print()\n",
    "    \n",
    "    # Create loss, optimizer and scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    \n",
    "    # # Train model\n",
    "    model, loss_train, acc_train, loss_val, acc_val = train_model(model, \n",
    "                                                                criterion, \n",
    "                                                                optimizer,\n",
    "                                                                scheduler,\n",
    "                                                                checkpoint,\n",
    "                                                                PATH_model_partial,\n",
    "                                                                i,\n",
    "                                                                num_epochs=epoch_tot)\n",
    "    \n",
    "    preds_vec, true_vec = prediciton(model)\n",
    "    \n",
    "    # # Save results\n",
    "    results = open(inputs.get('model_root')+inputs.get('model_results')+'logs_'+str(i), 'wb')\n",
    "    pickle.dump([loss_train,\n",
    "                acc_train,\n",
    "                loss_val,\n",
    "                acc_val,\n",
    "                preds_vec,\n",
    "                true_vec],\n",
    "                results)\n",
    "    \n",
    "    results.close()\n",
    "    \n",
    "    # # Save model for the training i \n",
    "    torch.save(model, inputs.get('model_root')+inputs.get('model_results')+'model_'+str(i))\n",
    "    \n",
    "    # We reach the end of the training, reset chepoint to None\n",
    "    checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Create resized dataset notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Description : \n",
    "\n",
    "Table of contents :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from cuttings import *\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def dataset_resized(test_name,\r\n",
    "                    label,\r\n",
    "                     list_images,\r\n",
    "                     date,\r\n",
    "                     size=128,\r\n",
    "                     train=True,\r\n",
    "                     test=False,\r\n",
    "                     folder_data=\"Cuttings_data/\"):\r\n",
    "    \r\n",
    "    test_path = test_name+\"/\"\r\n",
    "    \r\n",
    "    # List all the images in the folder\r\n",
    "    train_tiffs = os.listdir(folder_data+test_path)\r\n",
    "    \r\n",
    "    if train:\r\n",
    "        name_csv = 'train_'+date+'.csv'\r\n",
    "        path_cut = './train/cuttings_'+date+'/'\r\n",
    "        path_csv = './train/csv/'\r\n",
    "        \r\n",
    "    if test:\r\n",
    "        name_csv = 'test_'+date+'.csv'\r\n",
    "        path_cut = \"./test_new/cuttings_\"+date+\"/\" # Add new for the test set\r\n",
    "        path_csv = \"./test_new/csv/\" # Add new for the test set\r\n",
    "    \r\n",
    "    # create folders if not already existing\r\n",
    "    try :\r\n",
    "        os.listdir(path_cut)\r\n",
    "    except FileNotFoundError : \r\n",
    "        os.mkdir(path_cut) \r\n",
    "        print(\"Directory '%s' created\" %path_cut) \r\n",
    "    try :\r\n",
    "        os.listdir(path_csv)\r\n",
    "    except FileNotFoundError : \r\n",
    "        os.mkdir(path_csv) \r\n",
    "        print(\"Directory '%s' created\" %path_csv) \r\n",
    "        # Need to create empty csv file with all the features\r\n",
    "        df = pd.DataFrame(columns=['path',\r\n",
    "                                   'image_name',\r\n",
    "                                   'rock_type',\r\n",
    "                                   'scan_name',\r\n",
    "                                   'label_assigned',\r\n",
    "                                   'area',\r\n",
    "                                   'centroid',\r\n",
    "                                   'bbox',\r\n",
    "                                   'bbox_rect'])\r\n",
    "        df.to_csv(path_csv+name_csv)\r\n",
    "        \r\n",
    "    # Import CSV\r\n",
    "    df_metadata = pd.read_csv(path_csv+name_csv,index_col=0)\r\n",
    "    \r\n",
    "    WIDTH = size\r\n",
    "    HEIGHT = size\r\n",
    "    \r\n",
    "    # Iterate over all the images \r\n",
    "    for image_i in list_images :\r\n",
    "        \r\n",
    "        # Extract the image number\r\n",
    "        tiff = train_tiffs[image_i]\r\n",
    "        \r\n",
    "        cut = Cuttings(folder_data+test_path,tiff)\r\n",
    "        \r\n",
    "        image = cut.load_picture()\r\n",
    "        \r\n",
    "        mask = cut.assign_mask(image)\r\n",
    "        \r\n",
    "        dilated = cut.assign_label(mask)\r\n",
    "        \r\n",
    "        big_samples = cut.big_cuttings(dilated)\r\n",
    "        \r\n",
    "        label = label\r\n",
    "        \r\n",
    "        # Create dataframe to store the metadata for 1 scan\r\n",
    "        df_scan = pd.DataFrame()\r\n",
    "        \r\n",
    "        # Create lists to store the metadata for 1 scan\r\n",
    "        list_path = []\r\n",
    "        list_image_name = []\r\n",
    "        list_rock_type = []\r\n",
    "        list_scan_name = []\r\n",
    "        list_label_assigned = []\r\n",
    "        list_area = []\r\n",
    "        list_local_centroid = []\r\n",
    "        list_bbox = []\r\n",
    "        list_bbox_rect = []\r\n",
    "        \r\n",
    "        # Iterate on all the cuttings found on the pictures\r\n",
    "        for sample_i in big_samples:\r\n",
    "            \r\n",
    "            # Extract the cuttings\r\n",
    "            im_rect = img_as_ubyte(dilated == regionprops(dilated)[sample_i].label)\r\n",
    "            contours,_ = cv2.findContours(im_rect, 1, 2)\r\n",
    "            rect = cv2.minAreaRect(contours[0])\r\n",
    "            if len(contours) >= 2:\r\n",
    "                cmax = sorted(contours, key=cv2.contourArea, reverse=True)[0]\r\n",
    "                rect = cv2.minAreaRect(cmax)\r\n",
    "            box = cv2.boxPoints(rect)\r\n",
    "            # Rotated rectangle bounding box coordinates\r\n",
    "            box = np.int0(box)\r\n",
    "            \r\n",
    "            # Extract ractangular padded cutting\r\n",
    "            cutting_to_save = cv2.resize(crop_rectangle(image*im_rect,box,rect),\r\n",
    "                   (WIDTH,HEIGHT), \r\n",
    "                   interpolation=cv2.INTER_CUBIC)\r\n",
    "            \r\n",
    "            # Save cutting (original + mask)\r\n",
    "            # Cutting\r\n",
    "            cv2.imwrite(path_cut+test_name+'_'+train_tiffs[image_i][:-4]+'_'+str(sample_i)+'.png', cutting_to_save)\r\n",
    "\r\n",
    "            # Save metadata : \r\n",
    "            # Path\r\n",
    "            list_path.append(path_cut+test_name+'_'+train_tiffs[image_i][:-4]+'_'+str(sample_i)+'.png')\r\n",
    "            # Image name \r\n",
    "            list_image_name.append(test_name+'_'+train_tiffs[image_i][:-4]+'_'+str(sample_i))\r\n",
    "            # Rock type \r\n",
    "            list_rock_type.append(label)\r\n",
    "            # Scan name \r\n",
    "            list_scan_name.append(train_tiffs[image_i][:-4])\r\n",
    "            # Label_assigned\r\n",
    "            list_label_assigned.append(sample_i)\r\n",
    "            # Area\r\n",
    "            list_area.append(regionprops(dilated)[sample_i].area)\r\n",
    "            # Local Centroid\r\n",
    "            list_local_centroid.append(regionprops(dilated)[sample_i].local_centroid)\r\n",
    "            # Bbox\r\n",
    "            minr, minc, maxr, maxc = regionprops(dilated)[sample_i].bbox\r\n",
    "            list_bbox.append([minr, minc, maxr, maxc])\r\n",
    "            # Bbox_rect\r\n",
    "            minr_r, minc_r, maxr_r, maxc_r = list(box[0]),list(box[1]),list(box[2]),list(box[3])\r\n",
    "            list_bbox_rect.append([ minr_r, minc_r, maxr_r, maxc_r])\r\n",
    "            \r\n",
    "        df_scan['path'] = list_path\r\n",
    "        df_scan['image_name'] = list_image_name\r\n",
    "        df_scan['rock_type'] = list_rock_type\r\n",
    "        df_scan['scan_name'] = list_scan_name\r\n",
    "        df_scan['label_assigned'] = list_label_assigned\r\n",
    "        df_scan['area'] = list_area\r\n",
    "        df_scan['centroid'] = list_local_centroid\r\n",
    "        df_scan['bbox'] = list_bbox\r\n",
    "        df_scan['bbox_rect'] = list_bbox_rect\r\n",
    "        \r\n",
    "        df_metadata = df_metadata.append(df_scan,ignore_index=True)\r\n",
    "    df_metadata.to_csv(path_csv+name_csv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# for train test set\r\n",
    "#df_config = pd.read_excel('config_file.xlsx')\r\n",
    "#df_config.head()\r\n",
    "\r\n",
    "# For new test set\r\n",
    "df_config = pd.read_excel('config_file_test.xlsx')\r\n",
    "df_config.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  test_name  label  train  test  list_start  list_end  list_step\n",
       "0     ML-V1      0  False  True         100      1500         10\n",
       "1     ML-V2      0  False  True         100      1350         10\n",
       "2     MS-B1      1  False  True          50      1300         10\n",
       "3     MS-M1      1  False  True          50      1400         10\n",
       "4     MS-M2      1  False  True         100      1200         10"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>label</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>list_start</th>\n",
       "      <th>list_end</th>\n",
       "      <th>list_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ML-V1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML-V2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>1350</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS-B1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>1300</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS-M1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>1400</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MS-M2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>1200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dataset - 128x128"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "date = \"resized_128\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for i in range(df_config.shape[0]):\r\n",
    "    dataset_resized(df_config.loc[i,'test_name'],\r\n",
    "                df_config.loc[i,'label'],\r\n",
    "                np.arange(df_config.loc[i,'list_start'], df_config.loc[i,'list_end'], df_config.loc[i,'list_step']).tolist(),\r\n",
    "                date,\r\n",
    "                size=128,\r\n",
    "                train=df_config.loc[i,'train'],\r\n",
    "                test=df_config.loc[i,'test'],\r\n",
    "                folder_data='/Users/nilso/Documents/EPFL/MA4/PDS Turberg/Cuttings_data/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of rock per category :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_train = pd.read_csv('train/csv_'+date+'/train_'+date+'.csv',index_col=0)\r\n",
    "df_test = pd.read_csv('test/csv_'+date+'/test_'+date+'.csv',index_col=0)\r\n",
    "\r\n",
    "# Train\r\n",
    "print('Train')\r\n",
    "for i in range(5):\r\n",
    "    print('Number of rock type {} = {}'.format(i,df_train[df_train['rock_type'] == i].shape[0]))\r\n",
    "print()\r\n",
    "# Test\r\n",
    "print('Test')\r\n",
    "for i in range(5):\r\n",
    "    print('Number of rock type {} = {}'.format(i,df_test[df_test['rock_type'] == i].shape[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train\n",
      "Number of rock type 0 = 1846\n",
      "Number of rock type 1 = 1033\n",
      "Number of rock type 2 = 1660\n",
      "Number of rock type 3 = 1497\n",
      "Number of rock type 4 = 1374\n",
      "\n",
      "Test\n",
      "Number of rock type 0 = 709\n",
      "Number of rock type 1 = 517\n",
      "Number of rock type 2 = 559\n",
      "Number of rock type 3 = 656\n",
      "Number of rock type 4 = 398\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Create uniform dataset\r\n",
    "# Generate 1000 samples per rock type\r\n",
    "list_im_train = []\r\n",
    "random.seed(0)\r\n",
    "for i in range(5):\r\n",
    "    list_im_train+=random.sample(list(df_train[df_train['rock_type'] ==i].index.values),k = 1000)\r\n",
    "\r\n",
    "# Save training/validation dataset\r\n",
    "df_train.iloc[list_im_train].sort_index().reset_index(drop=True).to_csv('train/csv_'+date+'/train_'+date+'_final.csv')\r\n",
    "\r\n",
    "# Generate 200 samples per rock type\r\n",
    "list_im_test = []\r\n",
    "for i in range(5):\r\n",
    "    list_im_test+=random.sample(list(df_test[df_test['rock_type'] ==i].index.values),k = 200)\r\n",
    "\r\n",
    "# Save test dataset\r\n",
    "df_test.loc[list_im_test].sort_index().reset_index(drop=True).to_csv('test/csv_'+date+'/test_'+date+'_final.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_test = pd.read_csv('test_new/csv_'+date+'/test_'+date+'.csv',index_col=0)\r\n",
    "\r\n",
    "print('Test')\r\n",
    "for i in range(5):\r\n",
    "    print('Number of rock type {} = {}'.format(i,df_test[df_test['rock_type'] == i].shape[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test\n",
      "Number of rock type 0 = 2482\n",
      "Number of rock type 1 = 2404\n",
      "Number of rock type 2 = 0\n",
      "Number of rock type 3 = 0\n",
      "Number of rock type 4 = 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "list_im_test = []\r\n",
    "test_names = ['ML-V1','ML-V2','MS-B1','MS-M1','MS-M2']\r\n",
    "for i, name in enumerate(test_names):\r\n",
    "    list_im_test+=random.sample(list(df_test[df_test['image_name'].str[:5]==name].index.values),k = 200)\r\n",
    "\r\n",
    "# Save test dataset\r\n",
    "df_test.loc[list_im_test].sort_index().reset_index(drop=True).to_csv('test_new/csv_'+date+'/test_'+date+'_final.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute the Mean and Std for standarisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def MinMaxNormalization(image,vmin=0, vmax=1):\r\n",
    "        arr = np.array(image).astype('float32')\r\n",
    "        arr = (arr - arr.min()) / (arr.max() - arr.min())\r\n",
    "        arr = (vmax - vmin) * arr + vmin\r\n",
    "        return arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df = pd.read_csv('train/csv_'+date+'/'+'train_'+date+'_final.csv',index_col=0)\r\n",
    "\r\n",
    "vec_im = []\r\n",
    "for im in df['path']:\r\n",
    "    cut = Cuttings(im[:24],im[24:])\r\n",
    "    image = cut.load_picture()\r\n",
    "    image = MinMaxNormalization(image,vmin=0, vmax=1)\r\n",
    "    vec_im += list(image.reshape(-1))\r\n",
    "    \r\n",
    "print('Mean :')\r\n",
    "print(np.mean(vec_im))\r\n",
    "print()\r\n",
    "print('Std :')\r\n",
    "print(np.std(vec_im))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean :\n",
      "0.5157003\n",
      "\n",
      "Std :\n",
      "0.32948261\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df = pd.read_csv('test_new/csv_'+date+'/'+'test_'+date+'_final.csv',index_col=0)\r\n",
    "\r\n",
    "vec_im = []\r\n",
    "for im in df['path']:\r\n",
    "    cut = Cuttings(im[:24],im[24:])\r\n",
    "    image = cut.load_picture()\r\n",
    "    image = MinMaxNormalization(image,vmin=0, vmax=1)\r\n",
    "    vec_im += list(image.reshape(-1))\r\n",
    "\r\n",
    "print('--- Test New ---') \r\n",
    "print('Mean :')\r\n",
    "print(np.mean(vec_im))\r\n",
    "print()\r\n",
    "print('Std :')\r\n",
    "print(np.std(vec_im))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Test New ---\n",
      "Mean :\n",
      "0.3605516\n",
      "\n",
      "Std :\n",
      "0.26936868\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TORCH': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "c9bc2780ef9c74393fc13b2a6ebbb9325049e7560ec5b3fc49a36900b190151b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}